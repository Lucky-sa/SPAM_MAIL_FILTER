{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhFkcf_uL45U",
        "outputId": "e13e63d1-15f1-49ca-e906-07db3a2296e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9713004484304932\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/spam_mail_data.csv')\n",
        "# Remove NaN values\n",
        "df = df.dropna()\n",
        "# Adjust the training set values to not be empty and not contain errors like 'EOF inside string'\n",
        "df = df[df['Message'].apply(lambda x: len(x.strip()) > 0 and 'EOF inside string' not in x)]\n",
        "# Reset the index after dropping rows\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "# Define a function for text preprocessing\n",
        "def preprocess_text(text):\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove stopwords\n",
        "    tokens = [word for word in tokens if word.lower() not in stopwords.words('english')]\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word.lower()) for word in tokens]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Apply text preprocessing\n",
        "df['processed_email'] = df['Message'].apply(preprocess_text)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['processed_email'], df['Category'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a pipeline for text classification\n",
        "pipeline = Pipeline([\n",
        "    ('vect', CountVectorizer()),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf', MultinomialNB())\n",
        "])\n",
        "\n",
        "# Train the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print classification report\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fYg9hXXdVK7",
        "outputId": "7beae042-8451-46b0-ceb4-b72d11a2f1da"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.97      1.00      0.98       966\n",
            "        spam       1.00      0.79      0.88       149\n",
            "\n",
            "    accuracy                           0.97      1115\n",
            "   macro avg       0.98      0.89      0.93      1115\n",
            "weighted avg       0.97      0.97      0.97      1115\n",
            "\n"
          ]
        }
      ]
    }
  ]
}